{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4qkvLAnkp9x"
      },
      "source": [
        "# Supplement (bonus) to Additional Task (CMS and E2E): Vision Transformers for End-to-End Particle Identification with the CMS Experiment\n",
        "### Chenguang Guan\n",
        "\n",
        "1. In the task-1 notebook (\"E2E_and_CMS_task1_pytorch.ipynb\") and the first additional task notebook (\"E2E_and_CMS_Additional_Task_ViT.ipynb\"), we have applied pre-trained ViT and Swin Transformer to the E2E classification problem.\n",
        "\n",
        "2. In this notebook, we will compare ViT with MLP mixer, which is a supplement to the the first additional task notebook (\"E2E_and_CMS_Additional_Task_ViT.ipynb\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFhbEUiCwKh5"
      },
      "source": [
        "## I. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fwM201m560x",
        "outputId": "f5e3bd33-30e7-4c57-f64b-b13e472a9a19",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr  2 14:20:57 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0    42W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!/opt/bin/nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2llf81aoc1X",
        "outputId": "619dd8a8-5342-4447-c13e-198a7b5e6808",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z6JbsW9tCr9p"
      },
      "outputs": [],
      "source": [
        "!cp --recursive \"/content/gdrive/MyDrive/CMS_data\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qG7N3ePw9oG",
        "outputId": "fd525ae8-dac4-44c6-a501-2ba7c8b93f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iPugPqI5pJoE"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SDo9IfxRTHYT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_IYAN6qqGQ",
        "outputId": "f413f273-bcf0-4d7a-d14a-29eac84d0a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is: cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z09l1TXilWp9"
      },
      "source": [
        "## II. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OrOYVH3pTUn"
      },
      "outputs": [],
      "source": [
        "shuffle_list = np.arange(0, 249000*2, 1) \n",
        "random.shuffle(shuffle_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONjdq7GvKJo6"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/gdrive/MyDrive/CMS_data/shuffle_list.npy\",shuffle_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AqiCbEvhLT1S"
      },
      "outputs": [],
      "source": [
        "shuffle_list = np.load(\"/content/gdrive/MyDrive/CMS_data/shuffle_list.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N2E4S0mrICor"
      },
      "outputs": [],
      "source": [
        "f1 = h5py.File(r'/content/CMS_data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')\n",
        "f2 = h5py.File(r'/content/CMS_data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')\n",
        "total_X = np.concatenate( (f1[\"X\"][:], f2[\"X\"][:] ), axis=0 )\n",
        "total_y = np.concatenate( (f1[\"y\"][:], f2[\"y\"][:] ), axis=0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JdFr9POuJP6u"
      },
      "outputs": [],
      "source": [
        "total_X = total_X[shuffle_list]\n",
        "total_y = total_y[shuffle_list].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kM_WpGoN0z9O"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, shuffle_list, train=True, transform=None, target_transform=None):\n",
        "        global total_X, total_y\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train\n",
        "        self.length = len(total_X)\n",
        "        self.train_length = int( self.length * 9/10)\n",
        "        self.test_length = int( self.length / 10)\n",
        "            \n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.train_length\n",
        "        else:\n",
        "            return self.test_length\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        if self.train:\n",
        "            image, label = total_X[idx], total_y[idx]\n",
        "        else:\n",
        "            image, label = total_X[idx + self.train_length], total_y[idx + self.train_length]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L6C-9NvFisC3"
      },
      "outputs": [],
      "source": [
        "my_transform_normal = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPAh7klxisC3"
      },
      "outputs": [],
      "source": [
        "my_transform_pretrained = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnZ0Dyp0ja9x"
      },
      "outputs": [],
      "source": [
        "my_transform_single_channel = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)),\n",
        "                                   transforms.Lambda(lambda x: x.repeat(3, 1, 1) )\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0S2EbXVsqVlh"
      },
      "outputs": [],
      "source": [
        "train_set = MyDataset(shuffle_list=shuffle_list, train=True, transform=my_transform_normal)\n",
        "train_loader =  DataLoader(train_set, batch_size=500, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kpi9YN64iyOF"
      },
      "outputs": [],
      "source": [
        "test_set = MyDataset(shuffle_list=shuffle_list, train=False, transform=my_transform_normal)\n",
        "test_loader =  DataLoader(test_set, batch_size=500, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRpP1bkpn9L_"
      },
      "source": [
        "## III. Train and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_RaBVthUoAr9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LwNKVkC2qk71"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    running_loss = 0.\n",
        "    #total_sample = 0\n",
        "    for batch_i, data in enumerate(dataloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Compute prediction and loss\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        num_batch_print = 200\n",
        "        if batch_i % num_batch_print == num_batch_print - 1:\n",
        "            current = (batch_i + 1) * len(inputs)\n",
        "            print(f'batch: [{batch_i + 1:5d}], loss: {running_loss / num_batch_print:.3f}')\n",
        "            print(f'temporary loss: {loss.item():>7f} | [{current:>5d}/{size:>5d}]')\n",
        "            running_loss = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e2qgnoTkqr41"
      },
      "outputs": [],
      "source": [
        "def test_loop_Entropy(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    #size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            # get the inputs for test dataset\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            # calculate the outputs\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # classify which class the output in\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            #_, labels_value = torch.max(labels.data, 1)\n",
        "            \n",
        "            # obtain the statistics of test loss and correctness\n",
        "            test_loss += loss_fn(outputs, labels).item()\n",
        "            correct += (predicted == labels).sum().item() \n",
        "            #correct += (predicted == labels_value).sum().item() \n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100 * correct / total):>0.1f}%\")\n",
        "    print(f\"Avg loss: {test_loss / num_batches:>8f} \\n\")\n",
        "    return test_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7CuJFThl0yWc"
      },
      "outputs": [],
      "source": [
        "def test_loop_prob(dataloader, model):\n",
        "    model.eval()\n",
        "    #size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    total_prob = list()\n",
        "    total_label = list()\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for (i, data) in enumerate(dataloader):\n",
        "            # get the inputs for test dataset\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            # calculate the outputs\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)[:, 1]\n",
        "            \n",
        "            total_prob += torch.Tensor.cpu(probabilities).numpy().tolist()\n",
        "            \n",
        "            total_label += torch.Tensor.cpu(labels).numpy().tolist()\n",
        "            \n",
        "            if i % 25 == 0:\n",
        "                print(f\"Process: {i/num_batches*100}\\%\")\n",
        "    return np.array(total_prob), np.array(total_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlUgDkGmnc8N"
      },
      "source": [
        "## III. ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MIhaP4t28X2"
      },
      "source": [
        "The implementation is from https://github.com/lucidrains/vit-pytorch, where we change the prenorm to postnorm.\n",
        "\n",
        "Batch size: 500\n",
        "\n",
        "Training on: A100 GPU\n",
        "\n",
        "Learning Rate: if the test loss isn't improved for three consecutive epochs, the learning rate will be reduced by a factor of ten.\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Test Accuracy: 69.7% (Pre-norm); 69.9% (Post-norm)\n",
        "\n",
        "ROC-AUC score: 0.76 (Post-norm)\n",
        "\n",
        "The post-norm has slightly better performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class PostNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.norm( self.fn(x, **kwargs) )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n"
      ],
      "metadata": {
        "id": "PO_zbykLGED4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-norm"
      ],
      "metadata": {
        "id": "tbmExHfQNHee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_pre(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "lIpGbA-xOjfB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_pre(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer_pre(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "ajqaXb7-Nfye"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-norm"
      ],
      "metadata": {
        "id": "vRgPTV02NOQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_post(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PostNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PostNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "ILMrCrUMO8e2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_post(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer_post(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "zvwS12i9MzB1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OegdOug928X2"
      },
      "source": [
        "### Training and Accuracy (ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre-norm"
      ],
      "metadata": {
        "id": "I3O_hpv7TxMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ViT = ViT_pre(image_size=32, patch_size=4, num_classes=2, dim=32, depth=6, heads=8, mlp_dim=128, pool=\"cls\", channels=2, dim_head=32).to(device)"
      ],
      "metadata": {
        "id": "on7jKmBoQJlX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 20 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_ViT, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_ViT, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26b2b97-be73-4bd1-916a-77f313ec1c72",
        "id": "Z60ow5tRQJlY"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.692\n",
            "temporary loss: 0.665314 | [100000/448200]\n",
            "batch: [  400], loss: 0.668\n",
            "temporary loss: 0.651435 | [200000/448200]\n",
            "batch: [  600], loss: 0.655\n",
            "temporary loss: 0.649895 | [300000/448200]\n",
            "batch: [  800], loss: 0.645\n",
            "temporary loss: 0.654485 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 62.8%\n",
            "Avg loss: 0.654310 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.633\n",
            "temporary loss: 0.595667 | [100000/448200]\n",
            "batch: [  400], loss: 0.632\n",
            "temporary loss: 0.607458 | [200000/448200]\n",
            "batch: [  600], loss: 0.623\n",
            "temporary loss: 0.637537 | [300000/448200]\n",
            "batch: [  800], loss: 0.622\n",
            "temporary loss: 0.581627 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 66.0%\n",
            "Avg loss: 0.625156 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.615\n",
            "temporary loss: 0.622645 | [100000/448200]\n",
            "batch: [  400], loss: 0.617\n",
            "temporary loss: 0.628382 | [200000/448200]\n",
            "batch: [  600], loss: 0.613\n",
            "temporary loss: 0.607475 | [300000/448200]\n",
            "batch: [  800], loss: 0.612\n",
            "temporary loss: 0.607268 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.5%\n",
            "Avg loss: 0.613855 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.608\n",
            "temporary loss: 0.624736 | [100000/448200]\n",
            "batch: [  400], loss: 0.605\n",
            "temporary loss: 0.597865 | [200000/448200]\n",
            "batch: [  600], loss: 0.607\n",
            "temporary loss: 0.616042 | [300000/448200]\n",
            "batch: [  800], loss: 0.605\n",
            "temporary loss: 0.611647 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.4%\n",
            "Avg loss: 0.605160 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.600\n",
            "temporary loss: 0.589345 | [100000/448200]\n",
            "batch: [  400], loss: 0.600\n",
            "temporary loss: 0.574852 | [200000/448200]\n",
            "batch: [  600], loss: 0.600\n",
            "temporary loss: 0.585639 | [300000/448200]\n",
            "batch: [  800], loss: 0.603\n",
            "temporary loss: 0.592510 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.5%\n",
            "Avg loss: 0.603625 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.595\n",
            "temporary loss: 0.556994 | [100000/448200]\n",
            "batch: [  400], loss: 0.596\n",
            "temporary loss: 0.601151 | [200000/448200]\n",
            "batch: [  600], loss: 0.596\n",
            "temporary loss: 0.582972 | [300000/448200]\n",
            "batch: [  800], loss: 0.599\n",
            "temporary loss: 0.593315 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.9%\n",
            "Avg loss: 0.601514 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.594\n",
            "temporary loss: 0.593396 | [100000/448200]\n",
            "batch: [  400], loss: 0.593\n",
            "temporary loss: 0.620332 | [200000/448200]\n",
            "batch: [  600], loss: 0.593\n",
            "temporary loss: 0.613925 | [300000/448200]\n",
            "batch: [  800], loss: 0.597\n",
            "temporary loss: 0.602353 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.9%\n",
            "Avg loss: 0.597669 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.591\n",
            "temporary loss: 0.556309 | [100000/448200]\n",
            "batch: [  400], loss: 0.592\n",
            "temporary loss: 0.579653 | [200000/448200]\n",
            "batch: [  600], loss: 0.589\n",
            "temporary loss: 0.578663 | [300000/448200]\n",
            "batch: [  800], loss: 0.593\n",
            "temporary loss: 0.595158 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.9%\n",
            "Avg loss: 0.597721 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.587\n",
            "temporary loss: 0.620846 | [100000/448200]\n",
            "batch: [  400], loss: 0.590\n",
            "temporary loss: 0.574499 | [200000/448200]\n",
            "batch: [  600], loss: 0.589\n",
            "temporary loss: 0.603515 | [300000/448200]\n",
            "batch: [  800], loss: 0.587\n",
            "temporary loss: 0.555485 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.5%\n",
            "Avg loss: 0.594713 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.585\n",
            "temporary loss: 0.599904 | [100000/448200]\n",
            "batch: [  400], loss: 0.586\n",
            "temporary loss: 0.583518 | [200000/448200]\n",
            "batch: [  600], loss: 0.586\n",
            "temporary loss: 0.589211 | [300000/448200]\n",
            "batch: [  800], loss: 0.586\n",
            "temporary loss: 0.566577 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.2%\n",
            "Avg loss: 0.595836 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.573\n",
            "temporary loss: 0.562229 | [100000/448200]\n",
            "batch: [  400], loss: 0.573\n",
            "temporary loss: 0.565889 | [200000/448200]\n",
            "batch: [  600], loss: 0.572\n",
            "temporary loss: 0.536213 | [300000/448200]\n",
            "batch: [  800], loss: 0.571\n",
            "temporary loss: 0.555109 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.589347 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.567\n",
            "temporary loss: 0.603684 | [100000/448200]\n",
            "batch: [  400], loss: 0.569\n",
            "temporary loss: 0.548723 | [200000/448200]\n",
            "batch: [  600], loss: 0.570\n",
            "temporary loss: 0.573894 | [300000/448200]\n",
            "batch: [  800], loss: 0.570\n",
            "temporary loss: 0.545111 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.589900 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.567\n",
            "temporary loss: 0.562765 | [100000/448200]\n",
            "batch: [  400], loss: 0.567\n",
            "temporary loss: 0.579151 | [200000/448200]\n",
            "batch: [  600], loss: 0.567\n",
            "temporary loss: 0.577604 | [300000/448200]\n",
            "batch: [  800], loss: 0.566\n",
            "temporary loss: 0.565027 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.6%\n",
            "Avg loss: 0.591154 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.566\n",
            "temporary loss: 0.561408 | [100000/448200]\n",
            "batch: [  400], loss: 0.565\n",
            "temporary loss: 0.566595 | [200000/448200]\n",
            "batch: [  600], loss: 0.566\n",
            "temporary loss: 0.593488 | [300000/448200]\n",
            "batch: [  800], loss: 0.565\n",
            "temporary loss: 0.582791 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.589955 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.553258 | [100000/448200]\n",
            "batch: [  400], loss: 0.561\n",
            "temporary loss: 0.555175 | [200000/448200]\n",
            "batch: [  600], loss: 0.563\n",
            "temporary loss: 0.557089 | [300000/448200]\n",
            "batch: [  800], loss: 0.563\n",
            "temporary loss: 0.558739 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.590876 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.575793 | [100000/448200]\n",
            "batch: [  400], loss: 0.562\n",
            "temporary loss: 0.575940 | [200000/448200]\n",
            "batch: [  600], loss: 0.563\n",
            "temporary loss: 0.552813 | [300000/448200]\n",
            "batch: [  800], loss: 0.561\n",
            "temporary loss: 0.562425 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.7%\n",
            "Avg loss: 0.591249 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.533898 | [100000/448200]\n",
            "batch: [  400], loss: 0.561\n",
            "temporary loss: 0.578933 | [200000/448200]\n",
            "batch: [  600], loss: 0.562\n",
            "temporary loss: 0.544974 | [300000/448200]\n",
            "batch: [  800], loss: 0.560\n",
            "temporary loss: 0.547923 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.7%\n",
            "Avg loss: 0.591588 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.563159 | [100000/448200]\n",
            "batch: [  400], loss: 0.560\n",
            "temporary loss: 0.563933 | [200000/448200]\n",
            "batch: [  600], loss: 0.562\n",
            "temporary loss: 0.548853 | [300000/448200]\n",
            "batch: [  800], loss: 0.561\n",
            "temporary loss: 0.569390 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.591577 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.560\n",
            "temporary loss: 0.537618 | [100000/448200]\n",
            "batch: [  400], loss: 0.564\n",
            "temporary loss: 0.603268 | [200000/448200]\n",
            "batch: [  600], loss: 0.561\n",
            "temporary loss: 0.526007 | [300000/448200]\n",
            "batch: [  800], loss: 0.563\n",
            "temporary loss: 0.549683 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.7%\n",
            "Avg loss: 0.591603 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.538146 | [100000/448200]\n",
            "batch: [  400], loss: 0.561\n",
            "temporary loss: 0.578592 | [200000/448200]\n",
            "batch: [  600], loss: 0.561\n",
            "temporary loss: 0.556156 | [300000/448200]\n",
            "batch: [  800], loss: 0.561\n",
            "temporary loss: 0.552808 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.7%\n",
            "Avg loss: 0.591633 \n",
            "\n",
            "Done!\n",
            "Time Consumption 862.4808888435364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2e-sO85lQOd0"
      },
      "outputs": [],
      "source": [
        "path = \"./gdrive/MyDrive/CMS_model/DIY_ViT_pre_norm_weights.pth\"\n",
        "torch.save(model_ViT.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40c99c6-8833-47f4-9162-6d366ff10829",
        "id": "ATXwl_-MQOd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = \"./gdrive/MyDrive/CMS_model/DIY_ViT_pre_norm_weights.pth\"\n",
        "model_ViT.load_state_dict( torch.load(path) )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-norm"
      ],
      "metadata": {
        "id": "SlJREoeKPVDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ViT = ViT_post(image_size=32, patch_size=4, num_classes=2, dim=32, depth=6, heads=8, mlp_dim=128, pool=\"cls\", channels=2, dim_head=32).to(device)"
      ],
      "metadata": {
        "id": "ffuuDWfuFnMj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 20 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_ViT, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_ViT, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_ViT.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MamnPPqY6C-y",
        "outputId": "acac2f84-02b5-4b66-b2e2-120841545486"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.690\n",
            "temporary loss: 0.674187 | [100000/448200]\n",
            "batch: [  400], loss: 0.664\n",
            "temporary loss: 0.666566 | [200000/448200]\n",
            "batch: [  600], loss: 0.653\n",
            "temporary loss: 0.645730 | [300000/448200]\n",
            "batch: [  800], loss: 0.645\n",
            "temporary loss: 0.652268 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 64.4%\n",
            "Avg loss: 0.636669 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.634\n",
            "temporary loss: 0.647865 | [100000/448200]\n",
            "batch: [  400], loss: 0.632\n",
            "temporary loss: 0.622703 | [200000/448200]\n",
            "batch: [  600], loss: 0.627\n",
            "temporary loss: 0.567416 | [300000/448200]\n",
            "batch: [  800], loss: 0.626\n",
            "temporary loss: 0.621813 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 66.2%\n",
            "Avg loss: 0.622499 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.622\n",
            "temporary loss: 0.627090 | [100000/448200]\n",
            "batch: [  400], loss: 0.618\n",
            "temporary loss: 0.612087 | [200000/448200]\n",
            "batch: [  600], loss: 0.616\n",
            "temporary loss: 0.628523 | [300000/448200]\n",
            "batch: [  800], loss: 0.614\n",
            "temporary loss: 0.622118 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.4%\n",
            "Avg loss: 0.614864 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.612\n",
            "temporary loss: 0.603884 | [100000/448200]\n",
            "batch: [  400], loss: 0.610\n",
            "temporary loss: 0.614592 | [200000/448200]\n",
            "batch: [  600], loss: 0.612\n",
            "temporary loss: 0.626439 | [300000/448200]\n",
            "batch: [  800], loss: 0.610\n",
            "temporary loss: 0.612225 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.7%\n",
            "Avg loss: 0.611186 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.608\n",
            "temporary loss: 0.600868 | [100000/448200]\n",
            "batch: [  400], loss: 0.605\n",
            "temporary loss: 0.585909 | [200000/448200]\n",
            "batch: [  600], loss: 0.604\n",
            "temporary loss: 0.615064 | [300000/448200]\n",
            "batch: [  800], loss: 0.604\n",
            "temporary loss: 0.607842 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.3%\n",
            "Avg loss: 0.605598 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.600\n",
            "temporary loss: 0.618415 | [100000/448200]\n",
            "batch: [  400], loss: 0.601\n",
            "temporary loss: 0.628819 | [200000/448200]\n",
            "batch: [  600], loss: 0.603\n",
            "temporary loss: 0.578703 | [300000/448200]\n",
            "batch: [  800], loss: 0.602\n",
            "temporary loss: 0.576186 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.3%\n",
            "Avg loss: 0.605280 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.600\n",
            "temporary loss: 0.584241 | [100000/448200]\n",
            "batch: [  400], loss: 0.598\n",
            "temporary loss: 0.605396 | [200000/448200]\n",
            "batch: [  600], loss: 0.598\n",
            "temporary loss: 0.602137 | [300000/448200]\n",
            "batch: [  800], loss: 0.597\n",
            "temporary loss: 0.580077 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.8%\n",
            "Avg loss: 0.599791 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.595\n",
            "temporary loss: 0.602078 | [100000/448200]\n",
            "batch: [  400], loss: 0.595\n",
            "temporary loss: 0.596594 | [200000/448200]\n",
            "batch: [  600], loss: 0.597\n",
            "temporary loss: 0.557940 | [300000/448200]\n",
            "batch: [  800], loss: 0.594\n",
            "temporary loss: 0.577474 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.3%\n",
            "Avg loss: 0.596571 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.594\n",
            "temporary loss: 0.558967 | [100000/448200]\n",
            "batch: [  400], loss: 0.595\n",
            "temporary loss: 0.594534 | [200000/448200]\n",
            "batch: [  600], loss: 0.592\n",
            "temporary loss: 0.566094 | [300000/448200]\n",
            "batch: [  800], loss: 0.592\n",
            "temporary loss: 0.598879 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.7%\n",
            "Avg loss: 0.604568 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.592\n",
            "temporary loss: 0.543222 | [100000/448200]\n",
            "batch: [  400], loss: 0.592\n",
            "temporary loss: 0.597787 | [200000/448200]\n",
            "batch: [  600], loss: 0.592\n",
            "temporary loss: 0.557530 | [300000/448200]\n",
            "batch: [  800], loss: 0.591\n",
            "temporary loss: 0.589213 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.3%\n",
            "Avg loss: 0.596389 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.582\n",
            "temporary loss: 0.581759 | [100000/448200]\n",
            "batch: [  400], loss: 0.580\n",
            "temporary loss: 0.594523 | [200000/448200]\n",
            "batch: [  600], loss: 0.581\n",
            "temporary loss: 0.582799 | [300000/448200]\n",
            "batch: [  800], loss: 0.581\n",
            "temporary loss: 0.583220 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.589420 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.579\n",
            "temporary loss: 0.568564 | [100000/448200]\n",
            "batch: [  400], loss: 0.578\n",
            "temporary loss: 0.575166 | [200000/448200]\n",
            "batch: [  600], loss: 0.576\n",
            "temporary loss: 0.591987 | [300000/448200]\n",
            "batch: [  800], loss: 0.579\n",
            "temporary loss: 0.569382 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.588119 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.579\n",
            "temporary loss: 0.580789 | [100000/448200]\n",
            "batch: [  400], loss: 0.576\n",
            "temporary loss: 0.566724 | [200000/448200]\n",
            "batch: [  600], loss: 0.577\n",
            "temporary loss: 0.578888 | [300000/448200]\n",
            "batch: [  800], loss: 0.578\n",
            "temporary loss: 0.616859 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.588288 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.576\n",
            "temporary loss: 0.571008 | [100000/448200]\n",
            "batch: [  400], loss: 0.575\n",
            "temporary loss: 0.555593 | [200000/448200]\n",
            "batch: [  600], loss: 0.576\n",
            "temporary loss: 0.600369 | [300000/448200]\n",
            "batch: [  800], loss: 0.575\n",
            "temporary loss: 0.547621 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.589043 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.571\n",
            "temporary loss: 0.559766 | [100000/448200]\n",
            "batch: [  400], loss: 0.573\n",
            "temporary loss: 0.589434 | [200000/448200]\n",
            "batch: [  600], loss: 0.573\n",
            "temporary loss: 0.532087 | [300000/448200]\n",
            "batch: [  800], loss: 0.574\n",
            "temporary loss: 0.568937 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.587904 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.572\n",
            "temporary loss: 0.587954 | [100000/448200]\n",
            "batch: [  400], loss: 0.572\n",
            "temporary loss: 0.581788 | [200000/448200]\n",
            "batch: [  600], loss: 0.572\n",
            "temporary loss: 0.630443 | [300000/448200]\n",
            "batch: [  800], loss: 0.575\n",
            "temporary loss: 0.558955 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.587824 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.574\n",
            "temporary loss: 0.560816 | [100000/448200]\n",
            "batch: [  400], loss: 0.572\n",
            "temporary loss: 0.535443 | [200000/448200]\n",
            "batch: [  600], loss: 0.572\n",
            "temporary loss: 0.591409 | [300000/448200]\n",
            "batch: [  800], loss: 0.572\n",
            "temporary loss: 0.560483 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.587941 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.573\n",
            "temporary loss: 0.588249 | [100000/448200]\n",
            "batch: [  400], loss: 0.573\n",
            "temporary loss: 0.539778 | [200000/448200]\n",
            "batch: [  600], loss: 0.573\n",
            "temporary loss: 0.554891 | [300000/448200]\n",
            "batch: [  800], loss: 0.570\n",
            "temporary loss: 0.564472 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.8%\n",
            "Avg loss: 0.587888 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.574\n",
            "temporary loss: 0.587593 | [100000/448200]\n",
            "batch: [  400], loss: 0.571\n",
            "temporary loss: 0.567237 | [200000/448200]\n",
            "batch: [  600], loss: 0.571\n",
            "temporary loss: 0.584701 | [300000/448200]\n",
            "batch: [  800], loss: 0.573\n",
            "temporary loss: 0.577682 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.587887 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.573\n",
            "temporary loss: 0.578803 | [100000/448200]\n",
            "batch: [  400], loss: 0.572\n",
            "temporary loss: 0.568586 | [200000/448200]\n",
            "batch: [  600], loss: 0.573\n",
            "temporary loss: 0.565751 | [300000/448200]\n",
            "batch: [  800], loss: 0.571\n",
            "temporary loss: 0.577417 | [400000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.9%\n",
            "Avg loss: 0.587884 \n",
            "\n",
            "Done!\n",
            "Time Consumption 864.0547332763672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JxwnUG1uB3qz"
      },
      "outputs": [],
      "source": [
        "path = \"./gdrive/MyDrive/CMS_model/DIY_ViT_post_norm_weights01.pth\"\n",
        "torch.save(model_ViT.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LStD3Rj1xDDJ",
        "outputId": "1ef7ed3a-3b96-4dbd-f535-116864aac96c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "path = \"./gdrive/MyDrive/CMS_model/DIY_ViT_post_norm_weights01.pth\"\n",
        "model_ViT.load_state_dict( torch.load(path) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3WUfRD6u0kh"
      },
      "source": [
        "### ROC-AUC (ViT, post-norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Oa6-wVbPji",
        "outputId": "035b4f04-ce0e-43ce-bc02-3420aed409a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process: 0.0\\%\n",
            "Process: 25.0\\%\n",
            "Process: 50.0\\%\n",
            "Process: 75.0\\%\n"
          ]
        }
      ],
      "source": [
        "total_prob_ViT, total_label_ViT = test_loop_prob(test_loader, model_ViT)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(total_label_ViT, total_prob_ViT)\n",
        "ViT_AUC_1 = metrics.roc_auc_score(total_label_ViT, total_prob_ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6LsmkkObPjj",
        "outputId": "23969ade-64d7-4aaa-c456-274ecbcc1488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC-AUC score of Post-norm ViTis: 0.7560649162172641\n"
          ]
        }
      ],
      "source": [
        "print(\"The ROC-AUC score of Post-norm ViTis:\", ViT_AUC_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "MkfD04z3bPjj",
        "outputId": "ed5f89e5-7362-4549-ff5e-9d3ea782bd89"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(fpr, tpr, \"--\", color=\"magenta\")\n",
        "#plt.plot([0,1],[0,1],\"--\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC curve for Post-norm ViT\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV. MLP-mixer\n",
        "The MLP-mixer architecture comes from https://arxiv.org/abs/2105.01601 . We implement this architecture.\n",
        "\n",
        "Batch size: 200\n",
        "\n",
        "Training on: A100 GPU\n",
        "\n",
        "Learning Rate: if the test loss isn't improved for three consecutive epochs, the learning rate will be reduced by a factor of ten.\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Test Accuracy: 70.0%\n",
        "\n",
        "ROC-AUC score: 0.76\n",
        "\n",
        "MLP-mixer has comparable performance with ViT."
      ],
      "metadata": {
        "id": "erASHLeXQfff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5RyOV8lArlbi"
      },
      "outputs": [],
      "source": [
        "train_set = MyDataset(shuffle_list=shuffle_list, train=True, transform=my_transform_normal)\n",
        "train_loader =  DataLoader(train_set, batch_size=200, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZmXeG5j0rlbs"
      },
      "outputs": [],
      "source": [
        "test_set = MyDataset(shuffle_list=shuffle_list, train=False, transform=my_transform_normal)\n",
        "test_loader =  DataLoader(test_set, batch_size=200, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class PostNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.norm( self.fn(x, **kwargs) )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "x90y8oc_Z2E8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixerBlock(nn.Module):\n",
        "    def __init__(self, patch_dim, MLP_dim1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.MLP1 = nn.Sequential(\n",
        "            nn.Linear(patch_dim, MLP_dim1),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(MLP_dim1, patch_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = rearrange(x, \"b n p -> b p n\")\n",
        "        x = self.MLP1(x)\n",
        "        x = rearrange(x, \"b p n -> b n p\")\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, dim, MLP_dim2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.MLP2 = nn.Sequential(\n",
        "            nn.Linear(dim, MLP_dim2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(MLP_dim2, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.MLP2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "QjCHXHGtQ4Pd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_post(nn.Module):\n",
        "    def __init__(self, dim, patch_dim, depth, MLP_dim1, MLP_dim2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PostNorm(dim, MixerBlock(patch_dim, MLP_dim1) ),\n",
        "                PostNorm(dim, MLPBlock(dim, MLP_dim2) )\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for mixer, ff in self.layers:\n",
        "            x = mixer(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "iBmilkw2aKtN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_mixer_post(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, MLP_dim1, MLP_dim2, pool = 'cls', channels = 3):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "        self.transformer = Transformer_post(dim, num_patches + 1, depth, MLP_dim1, MLP_dim2)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "5VAMGGxMaKtN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mixer = MLP_mixer_post(image_size=32, patch_size=4, num_classes=2, dim=32, depth=6, MLP_dim1=128, MLP_dim2=128, channels=2).to(device)"
      ],
      "metadata": {
        "id": "NX3MblD3ZQN1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model_mixer.parameters(), lr=lr)\n",
        "lowest_loss = 1e4\n",
        "count = 0\n",
        "time_start = time.time()\n",
        "for t in range( 20 ):\n",
        "    print(f\"-------------Epoch {t+1}-------------\")\n",
        "    print(\"current learning rate\", lr)\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr_list[t], momentum=0.9)\n",
        "    train_loop(train_loader, model_mixer, criterion, optimizer)\n",
        "    test_loss = test_loop_Entropy(test_loader, model_mixer, criterion)\n",
        "    if int(test_loss * 100) < lowest_loss:\n",
        "        lowest_loss = int(test_loss * 100)\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 3:\n",
        "        lr /= 10\n",
        "        optimizer = optim.Adam(model_mixer.parameters(), lr=lr)\n",
        "        count = 0\n",
        "print(\"Done!\")\n",
        "time_end = time.time()\n",
        "print(\"Time Consumption\",time_end-time_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6766c3a-502e-4a1e-8c84-5e1c557b02c7",
        "id": "plP0H03SdVCs"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Epoch 1-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.698\n",
            "temporary loss: 0.673596 | [40000/448200]\n",
            "batch: [  400], loss: 0.676\n",
            "temporary loss: 0.652516 | [80000/448200]\n",
            "batch: [  600], loss: 0.662\n",
            "temporary loss: 0.648319 | [120000/448200]\n",
            "batch: [  800], loss: 0.655\n",
            "temporary loss: 0.635867 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.653\n",
            "temporary loss: 0.645348 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.650\n",
            "temporary loss: 0.615312 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.645\n",
            "temporary loss: 0.638919 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.641\n",
            "temporary loss: 0.662458 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.640\n",
            "temporary loss: 0.638867 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.636\n",
            "temporary loss: 0.640229 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.635\n",
            "temporary loss: 0.604948 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 64.9%\n",
            "Avg loss: 0.636690 \n",
            "\n",
            "-------------Epoch 2-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.632\n",
            "temporary loss: 0.604990 | [40000/448200]\n",
            "batch: [  400], loss: 0.630\n",
            "temporary loss: 0.601341 | [80000/448200]\n",
            "batch: [  600], loss: 0.630\n",
            "temporary loss: 0.636741 | [120000/448200]\n",
            "batch: [  800], loss: 0.626\n",
            "temporary loss: 0.693910 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.622\n",
            "temporary loss: 0.632573 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.623\n",
            "temporary loss: 0.590176 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.624\n",
            "temporary loss: 0.583711 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.624\n",
            "temporary loss: 0.688180 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.622\n",
            "temporary loss: 0.613796 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.620\n",
            "temporary loss: 0.611694 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.619\n",
            "temporary loss: 0.589800 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.0%\n",
            "Avg loss: 0.615855 \n",
            "\n",
            "-------------Epoch 3-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.617\n",
            "temporary loss: 0.600717 | [40000/448200]\n",
            "batch: [  400], loss: 0.617\n",
            "temporary loss: 0.630171 | [80000/448200]\n",
            "batch: [  600], loss: 0.617\n",
            "temporary loss: 0.614983 | [120000/448200]\n",
            "batch: [  800], loss: 0.616\n",
            "temporary loss: 0.623372 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.611\n",
            "temporary loss: 0.607619 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.615\n",
            "temporary loss: 0.632664 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.612\n",
            "temporary loss: 0.613603 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.612\n",
            "temporary loss: 0.590771 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.613\n",
            "temporary loss: 0.611952 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.611\n",
            "temporary loss: 0.651997 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.611\n",
            "temporary loss: 0.622481 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.4%\n",
            "Avg loss: 0.612216 \n",
            "\n",
            "-------------Epoch 4-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.608\n",
            "temporary loss: 0.616150 | [40000/448200]\n",
            "batch: [  400], loss: 0.608\n",
            "temporary loss: 0.622490 | [80000/448200]\n",
            "batch: [  600], loss: 0.607\n",
            "temporary loss: 0.660973 | [120000/448200]\n",
            "batch: [  800], loss: 0.607\n",
            "temporary loss: 0.530657 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.609\n",
            "temporary loss: 0.601206 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.609\n",
            "temporary loss: 0.594469 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.606\n",
            "temporary loss: 0.572497 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.603\n",
            "temporary loss: 0.622847 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.606\n",
            "temporary loss: 0.573406 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.605\n",
            "temporary loss: 0.638926 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.607\n",
            "temporary loss: 0.598946 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 67.7%\n",
            "Avg loss: 0.606827 \n",
            "\n",
            "-------------Epoch 5-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.605\n",
            "temporary loss: 0.559859 | [40000/448200]\n",
            "batch: [  400], loss: 0.602\n",
            "temporary loss: 0.622356 | [80000/448200]\n",
            "batch: [  600], loss: 0.606\n",
            "temporary loss: 0.610422 | [120000/448200]\n",
            "batch: [  800], loss: 0.604\n",
            "temporary loss: 0.655321 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.601\n",
            "temporary loss: 0.626992 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.599\n",
            "temporary loss: 0.629194 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.598\n",
            "temporary loss: 0.635102 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.600\n",
            "temporary loss: 0.604732 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.601\n",
            "temporary loss: 0.597835 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.601\n",
            "temporary loss: 0.555535 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.595\n",
            "temporary loss: 0.603504 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.6%\n",
            "Avg loss: 0.600318 \n",
            "\n",
            "-------------Epoch 6-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.602\n",
            "temporary loss: 0.544380 | [40000/448200]\n",
            "batch: [  400], loss: 0.598\n",
            "temporary loss: 0.604133 | [80000/448200]\n",
            "batch: [  600], loss: 0.596\n",
            "temporary loss: 0.580687 | [120000/448200]\n",
            "batch: [  800], loss: 0.598\n",
            "temporary loss: 0.580903 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.600\n",
            "temporary loss: 0.618717 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.596\n",
            "temporary loss: 0.611304 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.596\n",
            "temporary loss: 0.591017 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.595\n",
            "temporary loss: 0.630470 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.595\n",
            "temporary loss: 0.633645 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.594\n",
            "temporary loss: 0.584513 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.598\n",
            "temporary loss: 0.616954 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 68.6%\n",
            "Avg loss: 0.600581 \n",
            "\n",
            "-------------Epoch 7-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.593\n",
            "temporary loss: 0.547208 | [40000/448200]\n",
            "batch: [  400], loss: 0.593\n",
            "temporary loss: 0.557179 | [80000/448200]\n",
            "batch: [  600], loss: 0.594\n",
            "temporary loss: 0.600658 | [120000/448200]\n",
            "batch: [  800], loss: 0.593\n",
            "temporary loss: 0.551237 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.590\n",
            "temporary loss: 0.584652 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.591\n",
            "temporary loss: 0.562704 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.596\n",
            "temporary loss: 0.578725 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.594\n",
            "temporary loss: 0.553131 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.591\n",
            "temporary loss: 0.555146 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.593\n",
            "temporary loss: 0.595267 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.591\n",
            "temporary loss: 0.622799 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.0%\n",
            "Avg loss: 0.596128 \n",
            "\n",
            "-------------Epoch 8-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.588\n",
            "temporary loss: 0.565599 | [40000/448200]\n",
            "batch: [  400], loss: 0.588\n",
            "temporary loss: 0.622351 | [80000/448200]\n",
            "batch: [  600], loss: 0.593\n",
            "temporary loss: 0.591841 | [120000/448200]\n",
            "batch: [  800], loss: 0.588\n",
            "temporary loss: 0.584163 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.593\n",
            "temporary loss: 0.582663 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.587\n",
            "temporary loss: 0.560349 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.593\n",
            "temporary loss: 0.540536 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.589\n",
            "temporary loss: 0.648872 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.588\n",
            "temporary loss: 0.624575 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.589\n",
            "temporary loss: 0.613448 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.592\n",
            "temporary loss: 0.577551 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.0%\n",
            "Avg loss: 0.595420 \n",
            "\n",
            "-------------Epoch 9-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.584\n",
            "temporary loss: 0.573868 | [40000/448200]\n",
            "batch: [  400], loss: 0.585\n",
            "temporary loss: 0.621104 | [80000/448200]\n",
            "batch: [  600], loss: 0.588\n",
            "temporary loss: 0.611053 | [120000/448200]\n",
            "batch: [  800], loss: 0.582\n",
            "temporary loss: 0.613719 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.588\n",
            "temporary loss: 0.626508 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.586\n",
            "temporary loss: 0.571635 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.586\n",
            "temporary loss: 0.553745 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.583\n",
            "temporary loss: 0.585329 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.590\n",
            "temporary loss: 0.568522 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.588\n",
            "temporary loss: 0.563776 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.588\n",
            "temporary loss: 0.582909 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.0%\n",
            "Avg loss: 0.596078 \n",
            "\n",
            "-------------Epoch 10-------------\n",
            "current learning rate 0.001\n",
            "batch: [  200], loss: 0.584\n",
            "temporary loss: 0.610175 | [40000/448200]\n",
            "batch: [  400], loss: 0.582\n",
            "temporary loss: 0.584660 | [80000/448200]\n",
            "batch: [  600], loss: 0.583\n",
            "temporary loss: 0.596229 | [120000/448200]\n",
            "batch: [  800], loss: 0.582\n",
            "temporary loss: 0.598571 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.588\n",
            "temporary loss: 0.665086 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.581\n",
            "temporary loss: 0.554701 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.585\n",
            "temporary loss: 0.621338 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.588\n",
            "temporary loss: 0.612749 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.586\n",
            "temporary loss: 0.645185 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.585\n",
            "temporary loss: 0.585476 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.584\n",
            "temporary loss: 0.563873 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 69.5%\n",
            "Avg loss: 0.592107 \n",
            "\n",
            "-------------Epoch 11-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.574\n",
            "temporary loss: 0.600845 | [40000/448200]\n",
            "batch: [  400], loss: 0.573\n",
            "temporary loss: 0.565077 | [80000/448200]\n",
            "batch: [  600], loss: 0.574\n",
            "temporary loss: 0.568973 | [120000/448200]\n",
            "batch: [  800], loss: 0.571\n",
            "temporary loss: 0.543452 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.569\n",
            "temporary loss: 0.540776 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.566\n",
            "temporary loss: 0.592460 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.572\n",
            "temporary loss: 0.583763 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.569\n",
            "temporary loss: 0.573664 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.572\n",
            "temporary loss: 0.593691 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.568\n",
            "temporary loss: 0.581938 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.566\n",
            "temporary loss: 0.567630 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.584936 \n",
            "\n",
            "-------------Epoch 12-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.566\n",
            "temporary loss: 0.560638 | [40000/448200]\n",
            "batch: [  400], loss: 0.568\n",
            "temporary loss: 0.551832 | [80000/448200]\n",
            "batch: [  600], loss: 0.567\n",
            "temporary loss: 0.536888 | [120000/448200]\n",
            "batch: [  800], loss: 0.565\n",
            "temporary loss: 0.567567 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.570\n",
            "temporary loss: 0.575120 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.570\n",
            "temporary loss: 0.553517 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.563\n",
            "temporary loss: 0.525935 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.571\n",
            "temporary loss: 0.604393 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.567\n",
            "temporary loss: 0.547126 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.569\n",
            "temporary loss: 0.570081 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.562\n",
            "temporary loss: 0.570452 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.1%\n",
            "Avg loss: 0.585454 \n",
            "\n",
            "-------------Epoch 13-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.566\n",
            "temporary loss: 0.610844 | [40000/448200]\n",
            "batch: [  400], loss: 0.565\n",
            "temporary loss: 0.538357 | [80000/448200]\n",
            "batch: [  600], loss: 0.566\n",
            "temporary loss: 0.592386 | [120000/448200]\n",
            "batch: [  800], loss: 0.563\n",
            "temporary loss: 0.557151 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.565\n",
            "temporary loss: 0.604643 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.567\n",
            "temporary loss: 0.591819 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.564\n",
            "temporary loss: 0.530536 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.568\n",
            "temporary loss: 0.581275 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.568\n",
            "temporary loss: 0.544758 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.566\n",
            "temporary loss: 0.555945 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.565\n",
            "temporary loss: 0.561436 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586058 \n",
            "\n",
            "-------------Epoch 14-------------\n",
            "current learning rate 0.0001\n",
            "batch: [  200], loss: 0.558\n",
            "temporary loss: 0.557010 | [40000/448200]\n",
            "batch: [  400], loss: 0.564\n",
            "temporary loss: 0.563486 | [80000/448200]\n",
            "batch: [  600], loss: 0.562\n",
            "temporary loss: 0.588638 | [120000/448200]\n",
            "batch: [  800], loss: 0.565\n",
            "temporary loss: 0.556235 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.561\n",
            "temporary loss: 0.615448 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.563\n",
            "temporary loss: 0.569215 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.567\n",
            "temporary loss: 0.577500 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.564\n",
            "temporary loss: 0.597661 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.566\n",
            "temporary loss: 0.584382 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.566\n",
            "temporary loss: 0.562810 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.566\n",
            "temporary loss: 0.583696 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.585758 \n",
            "\n",
            "-------------Epoch 15-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.628485 | [40000/448200]\n",
            "batch: [  400], loss: 0.559\n",
            "temporary loss: 0.476486 | [80000/448200]\n",
            "batch: [  600], loss: 0.560\n",
            "temporary loss: 0.588959 | [120000/448200]\n",
            "batch: [  800], loss: 0.563\n",
            "temporary loss: 0.498441 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.559\n",
            "temporary loss: 0.560296 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.562\n",
            "temporary loss: 0.589967 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.565\n",
            "temporary loss: 0.485048 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.560\n",
            "temporary loss: 0.510464 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.561\n",
            "temporary loss: 0.574138 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.556\n",
            "temporary loss: 0.552703 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.562\n",
            "temporary loss: 0.517212 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.1%\n",
            "Avg loss: 0.586051 \n",
            "\n",
            "-------------Epoch 16-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.576659 | [40000/448200]\n",
            "batch: [  400], loss: 0.558\n",
            "temporary loss: 0.562766 | [80000/448200]\n",
            "batch: [  600], loss: 0.558\n",
            "temporary loss: 0.558509 | [120000/448200]\n",
            "batch: [  800], loss: 0.560\n",
            "temporary loss: 0.520508 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.559\n",
            "temporary loss: 0.588604 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.559\n",
            "temporary loss: 0.559881 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.560\n",
            "temporary loss: 0.634735 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.557\n",
            "temporary loss: 0.551897 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.565\n",
            "temporary loss: 0.556599 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.560\n",
            "temporary loss: 0.570117 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.563\n",
            "temporary loss: 0.620025 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586101 \n",
            "\n",
            "-------------Epoch 17-------------\n",
            "current learning rate 1e-05\n",
            "batch: [  200], loss: 0.560\n",
            "temporary loss: 0.568445 | [40000/448200]\n",
            "batch: [  400], loss: 0.559\n",
            "temporary loss: 0.579330 | [80000/448200]\n",
            "batch: [  600], loss: 0.563\n",
            "temporary loss: 0.563927 | [120000/448200]\n",
            "batch: [  800], loss: 0.560\n",
            "temporary loss: 0.596352 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.558\n",
            "temporary loss: 0.581651 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.559\n",
            "temporary loss: 0.560536 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.563\n",
            "temporary loss: 0.581891 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.560\n",
            "temporary loss: 0.581853 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.557\n",
            "temporary loss: 0.593037 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.564\n",
            "temporary loss: 0.503403 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.559\n",
            "temporary loss: 0.617278 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586418 \n",
            "\n",
            "-------------Epoch 18-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.557\n",
            "temporary loss: 0.508975 | [40000/448200]\n",
            "batch: [  400], loss: 0.556\n",
            "temporary loss: 0.586228 | [80000/448200]\n",
            "batch: [  600], loss: 0.560\n",
            "temporary loss: 0.553374 | [120000/448200]\n",
            "batch: [  800], loss: 0.560\n",
            "temporary loss: 0.647639 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.561\n",
            "temporary loss: 0.609575 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.555\n",
            "temporary loss: 0.601532 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.561\n",
            "temporary loss: 0.580267 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.564\n",
            "temporary loss: 0.618648 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.564\n",
            "temporary loss: 0.535213 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.559\n",
            "temporary loss: 0.618916 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.560\n",
            "temporary loss: 0.563787 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586455 \n",
            "\n",
            "-------------Epoch 19-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.569320 | [40000/448200]\n",
            "batch: [  400], loss: 0.564\n",
            "temporary loss: 0.589625 | [80000/448200]\n",
            "batch: [  600], loss: 0.562\n",
            "temporary loss: 0.503838 | [120000/448200]\n",
            "batch: [  800], loss: 0.556\n",
            "temporary loss: 0.552995 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.562\n",
            "temporary loss: 0.526028 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.558\n",
            "temporary loss: 0.540484 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.557\n",
            "temporary loss: 0.547654 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.558\n",
            "temporary loss: 0.567285 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.558\n",
            "temporary loss: 0.551170 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.559\n",
            "temporary loss: 0.544187 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.560\n",
            "temporary loss: 0.587021 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586511 \n",
            "\n",
            "-------------Epoch 20-------------\n",
            "current learning rate 1.0000000000000002e-06\n",
            "batch: [  200], loss: 0.562\n",
            "temporary loss: 0.564047 | [40000/448200]\n",
            "batch: [  400], loss: 0.562\n",
            "temporary loss: 0.607336 | [80000/448200]\n",
            "batch: [  600], loss: 0.556\n",
            "temporary loss: 0.508098 | [120000/448200]\n",
            "batch: [  800], loss: 0.561\n",
            "temporary loss: 0.570052 | [160000/448200]\n",
            "batch: [ 1000], loss: 0.558\n",
            "temporary loss: 0.593673 | [200000/448200]\n",
            "batch: [ 1200], loss: 0.562\n",
            "temporary loss: 0.584248 | [240000/448200]\n",
            "batch: [ 1400], loss: 0.560\n",
            "temporary loss: 0.563711 | [280000/448200]\n",
            "batch: [ 1600], loss: 0.559\n",
            "temporary loss: 0.599065 | [320000/448200]\n",
            "batch: [ 1800], loss: 0.559\n",
            "temporary loss: 0.516241 | [360000/448200]\n",
            "batch: [ 2000], loss: 0.560\n",
            "temporary loss: 0.598574 | [400000/448200]\n",
            "batch: [ 2200], loss: 0.560\n",
            "temporary loss: 0.604809 | [440000/448200]\n",
            "Test Error: \n",
            " Accuracy: 70.0%\n",
            "Avg loss: 0.586523 \n",
            "\n",
            "Done!\n",
            "Time Consumption 709.7474730014801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA76Uf1UwDNb"
      },
      "source": [
        "### ROC-AUC (MLP-mixer, post-norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff8f25d-3082-403d-8e63-c3230a8f0fa3",
        "id": "jGjnwWw9wDNl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process: 0.0\\%\n",
            "Process: 10.040160642570282\\%\n",
            "Process: 20.080321285140563\\%\n",
            "Process: 30.120481927710845\\%\n",
            "Process: 40.16064257028113\\%\n",
            "Process: 50.20080321285141\\%\n",
            "Process: 60.24096385542169\\%\n",
            "Process: 70.28112449799197\\%\n",
            "Process: 80.32128514056225\\%\n",
            "Process: 90.36144578313254\\%\n"
          ]
        }
      ],
      "source": [
        "total_prob_mixer, total_label_mixer = test_loop_prob(test_loader, model_mixer)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(total_label_mixer, total_prob_mixer)\n",
        "mixer_AUC_1 = metrics.roc_auc_score(total_label_mixer, total_prob_mixer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e379e4b-d7f9-4c3d-da2a-514a097fea5f",
        "id": "O8CnQicIwDNl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROC-AUC score of MLP-mixer: 0.7592256588781701\n"
          ]
        }
      ],
      "source": [
        "print(\"The ROC-AUC score of MLP-mixer:\", mixer_AUC_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b7f03b31-64bc-406c-ff68-9b95537aa3c9",
        "id": "d7o8aDUewDNl"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(fpr, tpr, \"--\", color=\"magenta\")\n",
        "#plt.plot([0,1],[0,1],\"--\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC curve for MLP-mixer\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}